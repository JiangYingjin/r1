# 实验配置
experiment:
  model: unsloth/Phi-3.5-mini-instruct-bnb-4bit
  # model: google/gemma-3-4b-it
  # model: Qwen/Qwen2.5-3B-Instruct
  # model: unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit
  name: better_reward_phi3.5  # 实验名称，可通过命令行参数覆盖
  description: |

# GPU 参数
gpu:
  memory_utilization: 0.7  # 若 OOM，可降低此值
  visible_devices: -1

# 训练参数
training:
  grpo_num_generations: 6  # 若 OOM，可降低此值
  total_steps: 300
  save_steps: 10
  learning_rate: 1e-5
  # learning_rate: 5e-6

# 随机种子
random_state: 9996

# LoRA 参数
lora:
  rank: 64  # 更大的秩 = 更智能，但更慢，建议选择 8, 16, 32, 64, 128

# 序列长度参数
sequence:
  max_prompt_length: 768
  max_completion_length: 2048

# 路径配置
paths:
  project_root: /root/proj/r1  # 项目根目录
  output_root: /root/lanyun-tmp/r1  # 项目输出目录

# 环境变量
environment:
  hf_hub_enable_hf_transfer: 0
  vllm_use_v1: 0

