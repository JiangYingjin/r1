ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 04-27 14:25:14 [__init__.py:239] Automatically detected platform cuda.
==((====))==  Unsloth 2025.4.1: Fast Gemma3 patching. Transformers: 4.51.3. vLLM: 0.8.4.
   \\   /|    NVIDIA GeForce RTX 2080 Ti. Num GPUs = 1. Max memory: 22.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Using float16 precision for gemma3 won't work! Using float32.
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Unsloth: Switching to float32 training since model cannot work with float16
GPU = NVIDIA GeForce RTX 2080 Ti. Max memory = 22.0 GB.
5.57 GB of memory reserved.
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2211, 'grad_norm': 0.8513830304145813, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 1.666, 'grad_norm': 1.5601744651794434, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 1.739, 'grad_norm': 1.247465968132019, 'learning_rate': 8e-05, 'epoch': 0.0}
{'loss': 1.3674, 'grad_norm': 0.927554190158844, 'learning_rate': 0.00012, 'epoch': 0.0}
{'loss': 1.1247, 'grad_norm': 0.9348872303962708, 'learning_rate': 0.00016, 'epoch': 0.0}
{'loss': 1.4371, 'grad_norm': 1.2464855909347534, 'learning_rate': 0.0002, 'epoch': 0.0}
{'loss': 0.7723, 'grad_norm': 0.5588536262512207, 'learning_rate': 0.000192, 'epoch': 0.0}
{'loss': 1.129, 'grad_norm': 0.9284756183624268, 'learning_rate': 0.00018400000000000003, 'epoch': 0.0}
{'loss': 0.9158, 'grad_norm': 0.4043347239494324, 'learning_rate': 0.00017600000000000002, 'epoch': 0.0}
{'loss': 0.8138, 'grad_norm': 0.4134770631790161, 'learning_rate': 0.000168, 'epoch': 0.0}
{'loss': 0.9334, 'grad_norm': 0.2910356819629669, 'learning_rate': 0.00016, 'epoch': 0.0}
{'loss': 1.1068, 'grad_norm': 0.40905889868736267, 'learning_rate': 0.000152, 'epoch': 0.0}
{'loss': 1.0089, 'grad_norm': 0.33727192878723145, 'learning_rate': 0.000144, 'epoch': 0.0}
{'loss': 0.6727, 'grad_norm': 0.328818142414093, 'learning_rate': 0.00013600000000000003, 'epoch': 0.0}
{'loss': 0.9255, 'grad_norm': 0.36496272683143616, 'learning_rate': 0.00012800000000000002, 'epoch': 0.0}
{'loss': 0.6897, 'grad_norm': 0.41101881861686707, 'learning_rate': 0.00012, 'epoch': 0.0}
{'loss': 1.0885, 'grad_norm': 0.4034048318862915, 'learning_rate': 0.00011200000000000001, 'epoch': 0.0}
{'loss': 0.8668, 'grad_norm': 0.41611865162849426, 'learning_rate': 0.00010400000000000001, 'epoch': 0.0}
{'loss': 0.8117, 'grad_norm': 0.2819909453392029, 'learning_rate': 9.6e-05, 'epoch': 0.0}
{'loss': 0.9947, 'grad_norm': 0.3336314260959625, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.0}
{'loss': 0.8809, 'grad_norm': 0.27340587973594666, 'learning_rate': 8e-05, 'epoch': 0.0}
{'loss': 0.8375, 'grad_norm': 0.25040096044540405, 'learning_rate': 7.2e-05, 'epoch': 0.0}
{'loss': 0.9995, 'grad_norm': 0.20276862382888794, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.0}
{'loss': 0.9502, 'grad_norm': 0.3489920198917389, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.0}
{'loss': 0.6531, 'grad_norm': 0.26269757747650146, 'learning_rate': 4.8e-05, 'epoch': 0.0}
{'loss': 0.8173, 'grad_norm': 0.2662240266799927, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 0.8347, 'grad_norm': 0.32544028759002686, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.0}
{'loss': 0.8192, 'grad_norm': 0.30693942308425903, 'learning_rate': 2.4e-05, 'epoch': 0.0}
{'loss': 1.0785, 'grad_norm': 0.3443532884120941, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}
{'loss': 1.0388, 'grad_norm': 0.30922967195510864, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'train_runtime': 197.1106, 'train_samples_per_second': 1.218, 'train_steps_per_second': 0.152, 'train_loss': 1.0064882973829905, 'epoch': 0.0}
197.1106 seconds used for training.
3.29 minutes used for training.
Peak reserved memory = 6.48 GB.
Peak reserved memory for training = 0.91 GB.
Peak reserved memory % of max memory = 29.455 %.
Peak reserved memory for training % of max memory = 4.136 %.
The sky is blue because of a phenomenon called Rayleigh scattering, which is the scattering of electromagnetic radiation (including light) by particles of a much smaller wavelength. Rayleigh scattering is a type of scattering in which the scattered light is emitted when electromagnetic radiation is scattered by particles in a medium. When sunlight enters the Earth's atmosphere
Gemma-3 is a family of lightweight, publicly available language models created by the team at Google DeepMind. Gemma-3 models come in a variety of sizes. Gemma-3 is known for being relatively accessible for developers and researchers to utilize.<end_of_turn>
